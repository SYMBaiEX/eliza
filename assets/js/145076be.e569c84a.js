"use strict";(self.webpackChunk_elizaos_docs=self.webpackChunk_elizaos_docs||[]).push([[47979],{4716:e=>{e.exports=JSON.parse('{"permalink":"/blog/openai-plugin-envs","editUrl":"https://github.com/elizaos/eliza/tree/v2-develop/docs/blog/openai-plugin-envs.md","source":"@site/blog/openai-plugin-envs.md","title":"Using OpenAI Plugin Envs for Any OpenAI-Compatible Provider","description":"Learn how to configure the OpenAI plugin to work with any OpenAI-compatible provider using environment variables.","date":"2025-04-24T00:00:00.000Z","tags":[],"readingTime":2.875,"hasTruncateMarker":false,"authors":[{"name":"ElizaOS Team","title":"Core Team","url":"https://github.com/elizaos","socials":{"twitter":"https://twitter.com/elizaOS","github":"https://github.com/elizaOS"},"imageURL":"https://github.com/elizaos.png","key":"team","page":null}],"frontMatter":{"title":"Using OpenAI Plugin Envs for Any OpenAI-Compatible Provider","authors":"team","date":"2025-04-24T00:00:00.000Z","description":"Learn how to configure the OpenAI plugin to work with any OpenAI-compatible provider using environment variables."},"unlisted":false,"lastUpdatedBy":"Sayo","prevItem":{"title":"Autofun Tokenomics","permalink":"/blog/autofun-tokenomics"},"nextItem":{"title":"Adding Plugins in V2","permalink":"/blog/add-plugins"}}')},65093:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>s,default:()=>p,frontMatter:()=>r,metadata:()=>o,toc:()=>d});var o=i(4716),l=i(31085),t=i(71184);const r={title:"Using OpenAI Plugin Envs for Any OpenAI-Compatible Provider",authors:"team",date:new Date("2025-04-24T00:00:00.000Z"),description:"Learn how to configure the OpenAI plugin to work with any OpenAI-compatible provider using environment variables."},s="\ud83d\ude80 Using OpenAI Plugin Envs Any OpenAI-Compatible Provider",a={authorsImageUrls:[void 0]},d=[{value:"\ud83e\udd14 What is an OpenAI-Compatible Provider?",id:"-what-is-an-openai-compatible-provider",level:2},{value:"\ud83d\udee0\ufe0f Key Environment Variables",id:"\ufe0f-key-environment-variables",level:2},{value:"Example: Connecting to OpenRouter",id:"example-connecting-to-openrouter",level:2},{value:"Example: Connecting to Ollama",id:"example-connecting-to-ollama",level:2},{value:"Example: Connecting to a Local LLM (Llama.cpp)",id:"example-connecting-to-a-local-llm-llamacpp",level:2},{value:"Example: Connecting to LM Studio",id:"example-connecting-to-lm-studio",level:2},{value:"\ud83d\udc1e Troubleshooting",id:"-troubleshooting",level:2},{value:"\ud83c\udfaf Conclusion",id:"-conclusion",level:2},{value:"\ud83d\udd17 More Links",id:"-more-links",level:2}];function c(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",em:"em",h2:"h2",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.code,{children:"plugin-openai"})," package in this project can connect to any OpenAI-compatible API provider\u2014not just OpenAI itself! Thanks to flexible environment variable support, you can swap between providers like Azure, OpenRouter, or even your own local LLM with just a few tweaks. It\u2019s that easy!"]}),"\n",(0,l.jsx)(n.h2,{id:"-what-is-an-openai-compatible-provider",children:"\ud83e\udd14 What is an OpenAI-Compatible Provider?"}),"\n",(0,l.jsx)(n.p,{children:"An OpenAI-compatible provider is any service that implements the OpenAI API spec. Popular examples include:"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://openrouter.ai/",children:"OpenRouter"})}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.a,{href:"https://ollama.com/",children:"Ollama"})," (with its OpenAI-compatible API)"]}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://github.com/abetlen/llama-cpp-python",children:"Local LLMs with an OpenAI API wrapper"})}),"\n",(0,l.jsx)(n.li,{children:"Other cloud or self-hosted endpoints"}),"\n"]}),"\n",(0,l.jsxs)(n.blockquote,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Note:"})," If your provider supports the OpenAI API, this plugin can probably talk to it!"]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"\ufe0f-key-environment-variables",children:"\ud83d\udee0\ufe0f Key Environment Variables"}),"\n",(0,l.jsx)(n.p,{children:"The following environment variables are supported by the OpenAI plugin:"}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Variable"}),(0,l.jsx)(n.th,{children:"Purpose"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"OPENAI_API_KEY"})}),(0,l.jsx)(n.td,{children:"The API key for authentication (required)"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"OPENAI_BASE_URL"})}),(0,l.jsx)(n.td,{children:"The base URL for the API (override to use other providers)"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"OPENAI_SMALL_MODEL"})}),(0,l.jsx)(n.td,{children:"Default small model name"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"OPENAI_LARGE_MODEL"})}),(0,l.jsx)(n.td,{children:"Default large model name"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"OPENAI_EMBEDDING_MODEL"})}),(0,l.jsx)(n.td,{children:"Embedding model name"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"OPENAI_EMBEDDING_DIMENSIONS"})}),(0,l.jsx)(n.td,{children:"Embedding vector dimensions"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"SMALL_MODEL"})}),(0,l.jsx)(n.td,{children:"(Fallback) Small model name"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.code,{children:"LARGE_MODEL"})}),(0,l.jsx)(n.td,{children:"(Fallback) Large model name"})]})]})]}),"\n",(0,l.jsx)(n.h2,{id:"example-connecting-to-openrouter",children:"Example: Connecting to OpenRouter"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-env",children:"OPENAI_API_KEY=your-openrouter-key\nOPENAI_BASE_URL=https://openrouter.ai/api/v1\nOPENAI_SMALL_MODEL=openrouter/gpt-3.5-turbo\nOPENAI_LARGE_MODEL=openrouter/gpt-4\n"})}),"\n",(0,l.jsx)(n.h2,{id:"example-connecting-to-ollama",children:"Example: Connecting to Ollama"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-env",children:"OPENAI_API_KEY=ollama-local-demo\nOPENAI_BASE_URL=http://localhost:11434/v1\nOPENAI_SMALL_MODEL=llama2\nOPENAI_LARGE_MODEL=llama2:70b\n"})}),"\n",(0,l.jsx)(n.h2,{id:"example-connecting-to-a-local-llm-llamacpp",children:"Example: Connecting to a Local LLM (Llama.cpp)"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-env",children:"OPENAI_API_KEY=sk-local-demo\nOPENAI_BASE_URL=http://localhost:8000/v1\nOPENAI_SMALL_MODEL=llama-2-7b-chat\nOPENAI_LARGE_MODEL=llama-2-13b-chat\n"})}),"\n",(0,l.jsx)(n.h2,{id:"example-connecting-to-lm-studio",children:"Example: Connecting to LM Studio"}),"\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.a,{href:"https://lmstudio.ai/",children:"LM Studio"})," is a popular desktop app for running large language models locally. It provides an OpenAI-compatible API server, so you can use it as a drop-in replacement for OpenAI or other providers."]}),"\n",(0,l.jsx)(n.p,{children:"To use LM Studio with the OpenAI plugin, start the LM Studio API server (default port 1234) and set your environment variables as follows:"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-env",children:"OPENAI_API_KEY=lmstudio-local-demo # (can be any non-empty string)\nOPENAI_BASE_URL=http://localhost:1234/v1\nOPENAI_SMALL_MODEL=your-model-name-here\nOPENAI_LARGE_MODEL=your-model-name-here\n"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["Make sure to use the model identifier as listed in LM Studio for the ",(0,l.jsx)(n.code,{children:"OPENAI_SMALL_MODEL"})," and ",(0,l.jsx)(n.code,{children:"OPENAI_LARGE_MODEL"})," values."]}),"\n",(0,l.jsxs)(n.li,{children:["LM Studio supports the ",(0,l.jsx)(n.code,{children:"/v1/models"}),", ",(0,l.jsx)(n.code,{children:"/v1/chat/completions"}),", ",(0,l.jsx)(n.code,{children:"/v1/embeddings"}),", and ",(0,l.jsx)(n.code,{children:"/v1/completions"})," endpoints."]}),"\n",(0,l.jsxs)(n.li,{children:["You can reuse any OpenAI-compatible SDK by pointing the base URL to ",(0,l.jsx)(n.code,{children:"http://localhost:1234/v1"}),"."]}),"\n"]}),"\n",(0,l.jsxs)(n.p,{children:["For more details, see the ",(0,l.jsx)(n.a,{href:"https://lmstudio.ai/docs/app/api/endpoints/openai",children:"LM Studio OpenAI Compatibility API docs"}),"."]}),"\n",(0,l.jsxs)(n.blockquote,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Reminder:"}),"\nAn embedding model is still required for full plugin functionality. We recommend pairing your setup with the Local AI plugin, an OpenAI API key, or a provider that also supports embeddings (such as OpenAI, OpenRouter, or LocalAI)."]}),"\n"]}),"\n",(0,l.jsx)(n.admonition,{type:"tip",children:(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Best Practice:"}),"\nKeep your ",(0,l.jsx)(n.code,{children:".env"})," file out of version control! Use environment variable management tools or deployment secrets for production."]})}),"\n",(0,l.jsxs)(n.blockquote,{children:["\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.strong,{children:"Reminder:"})}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["If ",(0,l.jsx)(n.code,{children:"OPENAI_BASE_URL"})," is not set, the plugin defaults to the official OpenAI endpoint."]}),"\n",(0,l.jsx)(n.li,{children:"Model names must match those supported by your provider."}),"\n",(0,l.jsx)(n.li,{children:"Some providers may require extra headers or parameters\u2014check their docs!"}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"-troubleshooting",children:"\ud83d\udc1e Troubleshooting"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["If you see errors about missing ",(0,l.jsx)(n.code,{children:"OPENAI_API_KEY"}),", make sure it\u2019s set in your environment."]}),"\n",(0,l.jsxs)(n.li,{children:["For provider-specific errors, double-check your ",(0,l.jsx)(n.code,{children:"OPENAI_BASE_URL"})," and model names."]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"-conclusion",children:"\ud83c\udfaf Conclusion"}),"\n",(0,l.jsx)(n.p,{children:"By customizing your environment variables, you can use the OpenAI plugin with any provider that supports the OpenAI API. Swap providers, use local models, or experiment with new services\u2014all with a few config changes. Happy hacking!"}),"\n",(0,l.jsx)(n.hr,{}),"\n",(0,l.jsx)(n.h2,{id:"-more-links",children:"\ud83d\udd17 More Links"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://eliza.how/docs/plugins",children:"Plugin Guide"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://platform.openai.com/docs/api-reference",children:"OpenAI API Reference"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://openrouter.ai/docs",children:"OpenRouter Docs"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://github.com/ollama/ollama/blob/main/docs/openai.md",children:"Ollama Docs"})}),"\n",(0,l.jsx)(n.li,{children:(0,l.jsx)(n.a,{href:"https://lmstudio.ai/docs/app/api/endpoints/openai",children:"LM Studio Docs"})}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:(0,l.jsx)(n.em,{children:"If you have questions or want to contribute more provider examples, feel free to open a PR!"})})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(c,{...e})}):c(e)}},71184:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>s});var o=i(14041);const l={},t=o.createContext(l);function r(e){const n=o.useContext(t);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:r(e.components),o.createElement(t.Provider,{value:n},e.children)}}}]);